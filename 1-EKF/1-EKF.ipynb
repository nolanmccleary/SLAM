{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivation and Abuse of the EKF Method for Real-World Slammage\n",
    "\n",
    "### Preface\n",
    "This guide page assumes a relative degree of comfort with probability and linear algebra. None of the math here is super nasty but there are a lot of steps so being comfortable with matrix operations (i.e. multiplications, what is a Transposition and why is it used, what is a Jacobian and why is it used), and clearly understanding what an expectation of a distribution is will go a long way. \n",
    "\n",
    "I also want to make the disclaimer that while robotics is used as the context for explaining why some of the math below works out the way it does, it's not the only way. EKF can be used for other things as well, but this whole piece is centered on explaining it in the context of SLAM. Therefore, robotics becomes the chosen medium.\n",
    "### Part 1 - The Derivation\n",
    "\n",
    "#### 1.0 - Preliminaries\n",
    "\n",
    "For now we will just focus on the robot's state, $x_k$. We will define the state vector distribution as some function of the previous state distribution, $f(x_{k-1})$, with zero-mean gaussian noise, $w_{k-1}$, added to it to account for jump uncertainty (for example, uncertainty in the path-planning algo). We will define our observation vector distribution, $z_k$, as some function of the current state, $f(x_k)$, with zero-mean gaussian noise, $v_{k}$, added to it to account for observation uncertainty (sensor noise). Our cookage manifests the following two equations:\n",
    "\n",
    "\n",
    ">$$\n",
    "x_k = f(x_{k-1}) + w_{k-1} \n",
    "\\\\\n",
    "z_k = f(x_k) + v_{k-1} \n",
    "\\\\\n",
    "$$\n",
    "\n",
    "We define the initial state, $x_0$, as a random vector with known mean $\\mu_0 = \\mathbb{E}[x_0]$ and covariance $P_0 = \\mathbb{E}\\left[(x_0 - \\mu_0)(x_0 - \\mu_0)^T\\right]$ \n",
    "\n",
    "From here, we are going to make a few assertions:\n",
    "\n",
    "1. Noise vector distributions have a zero-valued mean (I explained the intuition behind this in the previous part): $\\quad\\mathbb{E}[w_k] = 0, \\quad\\mathbb{E}[v_k] = 0$  \n",
    "\n",
    "2. The two types of noise vectors never correlate with each other: $\\quad\\mathbb{E}[w_k v_k^T j] = 0 \\quad \\forall k, j$  \n",
    "\n",
    "3. Neither noise vector has any correlation with the start point: $\\quad\\mathbb{E}[w_k x_0^T] = 0 \\quad \\mathbb{E}[v_k x_0^T] = 0 \\quad \\forall k$  \n",
    "\n",
    "4. Neither noise vector has any correlation whatsoever with any of its' predecessors or successors: $\\quad\\mathbb{E}[w_k w_j^T] = 0, \\quad \\mathbb{E}[v_k v_j^T] = 0 \\quad  \\forall k \\neq j$\n",
    "\n",
    "5. Vector variance (some sources use the term covariance but I think this is silly, because covariance with oneself is just variance) is represented with the following two matrices: $\\quad\\mathbb{E}[w_k w_k^T] = Q_k, \\quad \\mathbb{E}[v_k v_k^T] = R_k$\n",
    "\n",
    "\n",
    "We also make the assumption that functions $f(\\cdot)$ and $h(\\cdot)$ as well as their first-order derivatives are continuous on the given domain.\n",
    "\n",
    "To summarize, we have made a laundry-list of assertions to ensure that no Brimless Yankee activities will break what we do next. Dimensionality may be implictly known by now but nonetheless it is summarized below:\n",
    "\n",
    "\n",
    "$$\n",
    "x_k, \\quad n \\times 1 \\quad\\text{-- State vector at time step} \\space k \\newline\n",
    "w_k, \\quad n \\times 1 \\quad\\text{-- Process noise vector} \\newline\n",
    "z_k, \\quad m \\times 1 \\quad\\text{-- Observation vector at time step} \\space k \\newline\n",
    "v_k, \\quad m \\times 1 \\quad\\text{-- Measurement noise vector} \\newline\n",
    "f(\\cdot), \\quad n \\times 1 \\quad\\text{-- Process nonlinear vector function} \\newline\n",
    "h(\\cdot), \\quad m \\times 1 \\quad\\text{-- Observation nonlinear vector function} \\newline\n",
    "\\mathbb{Q_k}, \\quad n \\times n \\quad\\text{-- Process noise covariance matrix} \\newline\n",
    "\\mathbb{R_k}, \\quad m \\times m \\quad\\text{-- Measurement noise covariance matrix} \\newline\n",
    "$$\n",
    "\n",
    "\n",
    "> [!NOTE]\n",
    ">\n",
    ">In most (all?) real-world use-cases as far as SLAM is concerned, $n$ should always occupy a value between 1 and 3 inclusive. However, in order not to anger our high-dimensional overlords (the same mf's responsible for various lighthearted glitch-in-the-matrix activities such as ensuring that one runs into some very specific individual at very specific and unflattering times in a way and at a frequency that probabilistically just doesn't make sense), we are keeping $n$ abstractly defined. As you may imagine, $m$ is abstractly defined because it depends on the specifics of how the sensor suite collects data.\n",
    "\n",
    "#### 1.1 - Model Forecast Step\n",
    "\n",
    "This is analogous to the \"Time Update\" part of our generalized SLAM algorithm. In the beginning, the only information have is the mean, $\\mu_0$. We use this to obtain our initial optimal estimate, $x_0^a$ and variance, $\\mathbb{P_0}$ in the following manner:\n",
    "\n",
    "$$\n",
    "x_0^a = \\mu_0 = \\mathbb{E}[x_0]\n",
    "\\\\[5pt]\n",
    "\\mathbb{P_0} = \\mathbb{E}[(x_0 - x_0^a)(x_0 - x_0^a)^T]\n",
    "$$\n",
    "\n",
    "This is intuitive for our initial optimal estimate because we would expect it to be at the starting point that is by definition the most likely. We don't condition on any observation for this step because we operate under the assumption that $\\mu_0$ is obtained either through prior contextual knowledge, or an initial guess. Of course, that intial knowledge/guess could be obtained (at least in part) through sensor readings, but then it wouldn't really make sense to condition it on those same sensor readings thereafter. For peace of mind, we will assume that our overlords gift us this information via high-dimensional carrier pigeon, and any sensor readings thereafter would only add impurity to the divine estimate. We will not pain ourselves with the specifics of how it was obtained any further.\n",
    "\n",
    "\n",
    "We are only gifted one high-dimensional carrier piegon, however. This means that, assuming our sensor data is on average more accurate than our odometry data, the following states' ideal estimates should incorporate conditioning from the sensor data in order to be ideal.\n",
    "\n",
    "$$\n",
    "x_{k-1}^a \\equiv \\mathbb{E}[x_{k-1} | z_{k-1}]\n",
    "$$\n",
    "\n",
    "Now, for reasons that are not yet obvious, let's say we want to get Taylor's Version (Taylor Aproximation) of $f(x_{k-1})$. We can do that as follows:\n",
    "\n",
    "$$f(x_{k-1}) \\equiv f(x_{k-1}^a) + \\mathbb{J_f}(x_{k-1} - x_{k-1}^a) + H.O.T.$$\n",
    "\n",
    "Where the Jacobian, $\\mathbb{J_f}$, is defined as:\n",
    "\n",
    "$$\n",
    "J_f = \\begin{bmatrix}\n",
    "\\frac{\\partial f_1(x)}{\\partial x_1} & \\frac{\\partial f_1(x)}{\\partial x_2} & \\cdots & \\frac{\\partial f_1(x)}{\\partial x_n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial f_n(x)}{\\partial x_1} & \\frac{\\partial f_n(x)}{\\partial x_2} & \\cdots & \\frac{\\partial f_n(x)}{\\partial x_n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In the spirit of Taylor, we simplify for the masses:\n",
    "\n",
    "$$f(x_{k-1}) \\approx f(x_{k-1}^a) + \\mathbb{J_f}e_{k-1}$$\n",
    "\n",
    "Where $e_{k-1} \\equiv x_{k-1} - x_{k-1}^a$ and the higher-order terms are dropped because they have negligible effect on the result. Now we have a convenient way of estimating $f(x_{k-1})$ knowing only $f(x_{k-1}^a)$, $x_{k-1}^a$, and $x_{k-1}$. As you may expect, at this point this is completely useless because we dont know $x_{k-1}$, and we also wouldn't know $x_{k-1}^a$ unless $k-1 = 0$. As you may expect in addition to the previously expected, all following math in this document is dedicated towards ultimately rectifying this.\n",
    "\n",
    "Now what we want to do is figure out how to get the most accurate possible 'time update' value for $x_k$, which we denote as $x_k^f$. To do this we consider the Markov (Bumbling) process from part 0:\n",
    "1. Move to a new state $x_k$ as a function of the previous state $f(x_{k-1})$\n",
    "2. Make an observation $z_k$ that is probably more accurate than our odometry data.\n",
    "\n",
    "From this, it's relatively easy to see that the best way we could get an estimate for $x_k^f$ would be to take the expectation of $f(x_{k-1})$ conditioned on $z_{k-1}$. Doing so yields the following:\n",
    "\n",
    "$$\n",
    "x_k^f = \\mathbb{E}[f(x_{k-1}) | z_{k-1}] \\approx \\mathbb{E}[(f(x_{k-1}^a) + \\mathbb{J_f}(x_{k-1}^a)e_{k-1}) | z_{k-1}]\\\\[8pt]\n",
    "\n",
    "= f(x_{k-1}^a) + \\mathbb{J_f(x_{k-1}^a)}\\mathbb{E}[e_{k-1} | z_{k-1}]\\\\[10pt]\n",
    "\n",
    "\\therefore x_k^f \\approx f(x_{k-1}^a)\n",
    "$$\n",
    "\n",
    "\n",
    "The expectation of $f(x_{k-1}^a) | z_{k-1}$ is $f(x_{k-1}^a)$ because $x_{k-1}^a$ is itself an expectation conditioned on $z_{k-1}$.\n",
    "\n",
    "The expectation of $e_{k-1} | z_{k-1}$ is zero because we expect, on average, a delta of zero between our ideal estimate and our actual value. As with $x_{k-1}^a$, this delta will already have the observation conditioning baked in. If this isn't intuitive, review unbiased estimators.\n",
    "\n",
    ">[!NOTE]\n",
    ">\n",
    ">Of course, sensor bias does exist in the real world but for now we will assume that either it doesn't, or that we can compensate for it in a deterministic manner.\n",
    "\n",
    "Now we have a decent enough approximation for $x_k^f$, which is itself a function of our best possible approximation for $x_{k-1}$, but now we run into a problem: $x_k$ is a distribution, not a single number like $x_k^f$. In addition to this, it's not like we really knew what our $x_{k-1}$ was in the first place, we just took out best guess, $x_{k-1}^a$. \n",
    "\n",
    "This means that to get our current position distribution, $x_k$, we have to add noise back into the equation. This noise represents the uncertainty from our previous state estimations that is propagated through the process. Our final equation looks like this:\n",
    "\n",
    "$$\n",
    "x_k = x_k^f + w_{k-1} \\approx f(x_{k-1}^a) + w_{k-1}\n",
    "$$\n",
    "\n",
    "The noise has a zero-mean as mentioned earlier. You can rationalize this to yourself by asserting that on average, our previous state estimations will more or less be accurate due to the fact that our estimator (driven by unbiased sensor data) is unbiased, and will therefore have an overall average deviation of 0 from the true value. \n",
    "\n",
    "The next thing we have to do is take $x_k$ and condition on our observation $z_k$ in order to obtain the ideal estimate used for the next time step of the process, $x_k^a$.\n",
    "\n",
    "### Part 2 - The Abuse\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
